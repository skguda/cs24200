Jupyter NotebookHomework3 Last Checkpoint: 10/01/2020 (autosaved)Python 3 [3.6]
# Homework 3
Deadline: September 30, 2020 at 11:59pm EST.

For this homework, you cannot use the python library scikit-learn (sklearn).

In this assignment, you will construct a document-term matrix, which is a 2-dimensional matrix that describes the frequency of terms (i.e., words) that occur in each document in a collection. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms. The value in a given cell (i, j) represents the number of times the term j occurs in document i. Reference: https://en.wikipedia.org/wiki/Document-term_matrix

Q1: Feature Counting (10)
The text file 'urls.txt' contains a list of urls for the webpages to be parsed. Each line in the text file corresponds to a specific url. Use BeautifulSoup to fetch each webpage and parse the text as you did in HW1. Specifically,

For each webpage document, read the content and do the following:

A. Retrieve all text enclosed in paragraph tags. B. Convert to lowercase. C. Strip out punctuation. Note: if you use translate() with string.punctuation, then it may not strip out all characters. Use a regular expression to remove non-alphanumeric characters. D. Exclude all stop words given in the file 'stop_words.txt'.

Find the set of unique words across all documents and sort them in lexicographic order. This comprises the "vocabulary" of the corpus. Each term in the vocabulary will be a feature in the document-term matrix you will construct next. Do not add the empty string to the vocabulary.
Construct the document-term matrix, where the value of the cell (i, j) in the matrix is the frequency of the term j in document i. The number of columns of the document-term matrix will correspond to the number of unique words in the vocabulary, and the number of rows will correspond to the number of documents (i.e., webpages in 'urls.txt'.) Each term is represented by a column of the document-term matrix. Each document is represented by a row of the document-term matrix (ordered in the same way as the urls in the file.)
Select the most common word from each document to report in an output file named 'Q1.txt' for evaluation.
First, report the total vocabulary size.

Then, write the most frequently occurring word from each document along with its frequency in each of the documents in the format given below:

< Most Freq Word in Doc 1 > [< Freq in Doc1 > < Freq in Doc2 > ...] < tf of Most Freq Word in Doc 1 > < Most Freq Word in Doc 2 > [ < Freq in Doc1 > < Freq in Doc2 > ...] < tf of Most Freq Word in Doc 2 > ...

Example Output:

data [111 108 2 69 35] 0.06 word [111 108 0 69 35] 0.05

Note that your answer should start like this, or something close enough.

Pay close attention to the format. Use whitespaces as shown, do not add commas.

from bs4 import BeautifulSoup 
import urllib 
import re 
​
### YOUR CODE HERE
urls = open('urls.txt', 'r') 
url_list = [] 
for url in urls: 
    url_list.append(url.strip()) 
​
pgraph_list = [] 
#count = 0 
stop = open('stop_words.txt', 'r') 
stop_list = [] 
for line in stop: 
    stop_list.append(line.strip().lower())
for url in url_list:   
# Web parsing 
    req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
    webUrl = urllib.request.urlopen(req).read() 
    soup = BeautifulSoup(webUrl, 'lxml')  
    paragraphs = soup.find_all('p')   
    pgraphs = '' 
# Extracting paragraphs from webpages
    for p in paragraphs:
        ptext = p.getText().strip().lower() 
        pgraphs += ptext + ' '  
        
    rec = re.compile(r'\w+|\d+') 
    totalText = pgraphs.strip() 
    words = rec.findall(totalText) 
    wlist = [] 
    
    for word in words:
        if word not in stop_list: 
            wlist.append(word)  
#Dumping all word lists from each URL into one master list
    pgraph_list.append(wlist)  
urls.close() 
stop.close()
​
#print(pgraph_list)   
​
dict_list = []
for word_list in pgraph_list:  
    worddict = {} 
    for item in word_list: 
        if item not in worddict and item != '': 
            worddict[item] = 1 
        else: 
            worddict[item] += 1  
# List of dictionaries with all words counts of the URLS             
    dict_list.append(sorted(worddict.items(), key = lambda x:x[1], reverse = True))      
 
    
#super_dict = {} 
#for item in dict_list: 
#    for pair in item: 
#        for word in pair:
#            if word not in super_dict: 
#                super_dict[word] = 1
#            else: 
#                super_dict[word] += 1                 
​
unique_list = [] 
for item in dict_list: 
    sub_unique = []
    for tup in item:  
        # Adds both unique words (words which occur once) as well as not so unique words
        #if tup[0] not in sub_unique:  
        sub_unique.append(tup[0])  
    sub = sorted(sub_unique)
    unique_list.append(sub) 
​
# Set of unique words    
unique_set = [] 
for item in unique_list: 
    for word in item:
        if word not in unique_set and word != '': 
            unique_set.append(word)
​
new_unique_set = sorted(unique_set)  
# Total amount of vocab words
vocabulary_size = len(new_unique_set)  
 
# List of each document's words
doc_lengths = [] 
for dicti in dict_list: 
    some = 0
    for item in dicti: 
        #for i in range(0, len(item)): 
        some += item[1] 
    doc_lengths.append(some) 
    
# Finding most common word across each dict list
mostCommonWords = [] 
for item in dict_list: 
    for i in range(0, 1): 
        for j in range(0,1): 
            mostCommonWords.append(item[i][0])  
​
# Finding frequence of mostCommon words across each url   
cwFreq = [] 
for num in range(0, len(mostCommonWords)): 
    small_list = []
    for dicto in pgraph_list: 
        small_list.append(dicto.count(mostCommonWords[num]))
    cwFreq.append(small_list)           
    
# Term Frequency in the docs
tf_list = [] 
for i in range(0, len(cwFreq)):  
    tf_list.append(cwFreq[i][i] / doc_lengths[i])    
​
#final output    
finfile = open('Q1.txt', 'w')  
​
finfile.write(str(vocabulary_size) + ' ') 
​
for i in range(0, len(cwFreq)): 
    finfile.write(str(mostCommonWords[i]) + ' '+ str(cwFreq[i]) + ' ' + str(tf_list[i]) + ' ') 
finfile.close() 
ffile = open('Q1.txt', 'r') 
for line in ffile: 
    print(line.strip())
ffile.close() 
​
#print(cwFreq)
#print(doc_lengths)
1900 data [69, 108, 0, 119, 35, 6] 0.10391566265060241 data [69, 108, 0, 119, 35, 6] 0.09800362976406533 science [39, 22, 16, 15, 26, 3] 0.041884816753926704 data [69, 108, 0, 119, 35, 6] 0.07155742633794347 data [69, 108, 0, 119, 35, 6] 0.044359949302915085 purdue [0, 0, 6, 0, 0, 8] 0.0425531914893617
Q2: Grouping Features (5)
Using the document-term matrix obtained from Q1, group the words below that have a common prefix/stem.

i. 'write', 'writing', 'wrote', 'writes', 'written'
ii. 'return', 'returns', 'returned', 'returning'
iii. 'science', 'sciences', 'scientific', 'scientist', 'scientists'
iv. 'use', 'usage', 'used', 'user', 'uses', 'using'
v. 'work', 'worker', 'working', 'worked', 'works', 'workers'

Find all occurences of the above terms across all the documents. For each group above, sum the frequencies of the associated terms for each document.

Output the results in a file named 'Q2.txt' for evaluation. The format of the output file is given below:

_words_from_i [< summed_freq_in_doc1 >, ...]
words_from_ii [< summed_freq_indoc1 >, ...]
...

Example:

['write', 'writing', 'wrote', 'writes', 'written'] [1, 0, 0, 0, 0]
['return', 'returns', 'returned', 'returning'] [0, 2, 1, 0, 0]

The above values are for the first three webpage urls given in the file 'urls.txt'. The resulting sum for a word, for each document must be in the order of the urls given in 'urls.txt'.

from bs4 import BeautifulSoup 
import urllib 
import re
### YOUR CODE HERE 
main_list = [['write', 'writing', 'wrote', 'writes', 'written'], ['return', 'returns', 'returned', 'returning'], ['science', 'sciences', 'scientific', 'scientist', 'scientists'], [ 'use', 'usage', 'used', 'user', 'uses', 'using'], ['work', 'worker', 'working', 'worked', 'works', 'workers']] 
​
urls = open('urls.txt', 'r') 
url_list = [] 
for url in urls: 
    url_list.append(url.strip()) 
​
pgraph_list = [] 
#count = 0 
stop = open('stop_words.txt', 'r') 
stop_list = [] 
for line in stop: 
    stop_list.append(line.strip().lower())
for url in url_list:   
    #Web parsing 
    req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
    webUrl = urllib.request.urlopen(req).read() 
    soup = BeautifulSoup(webUrl, 'lxml')  
    paragraphs = soup.find_all('p')   
    pgraphs = '' 
    # Extracting paragraphs from webpages
    for p in paragraphs:
        ptext = p.getText().strip().lower() 
        pgraphs += ptext + ' ' 
    rec = re.compile(r'\w+|\d+') 
    totalText = pgraphs.strip() 
    words = rec.findall(totalText) 
    wlist = []
    for word in words:
        if word not in stop_list: 
            wlist.append(word)
#Dumping all word lists from each URL into one master list
    pgraph_list.append(wlist)  
urls.close() 
#stop.close() 
​
combo_list = []
for i in range(0, len(main_list)): 
    combo_list.append([])
    
#print(pgraph_list)
for graph in pgraph_list: 
    for i in range(0, len(combo_list)): 
        word_sum = 0
        for word in main_list[i]: 
            word_sum += graph.count(word) 
        combo_list[i].append(word_sum) 
        
file = open('Q2.txt', 'w')
for j in range(0, len(combo_list)): 
    file.write(str(main_list[j]) + ' ' + str(combo_list[j]) + '\n') 
file.close() 
file1 = open('Q2.txt', 'r') 
for line in file1: 
    print(line.strip())
​
#print(string_count.strip())    
['write', 'writing', 'wrote', 'writes', 'written'] [2, 0, 0, 5, 0, 0]
['return', 'returns', 'returned', 'returning'] [0, 2, 0, 0, 0, 0]
['science', 'sciences', 'scientific', 'scientist', 'scientists'] [50, 49, 17, 67, 26, 6]
['use', 'usage', 'used', 'user', 'uses', 'using'] [8, 13, 0, 19, 32, 0]
['work', 'worker', 'working', 'worked', 'works', 'workers'] [0, 5, 1, 6, 2, 0]
Q3: Feature Transformation (5)
For every word, find its average frequency across all documents.
Create a new document-term matrix with the same dimensions as above. For the value of cell (i, j), if the frequency of word j in document i is greater than the average frequency of word j, then set the value to +1, otherwise set it to -1.
Calculate the idf of the word j across all documents.
Select a subset (as described below) of this transformed document-term matrix to report in an output file named 'Q3.txt' for evaluation.
For the words: data, companies, business, action, mining, and science, write the words along with their corresponding values for each document to the output file.

Each line corresponds to one of the target words. Each line should follow the format given below:

word < avg > < [ list of 1,-1 with respect to document ] > < idf of this word >

Please write the words in the order that they are given above.

Example:

data 64.6 [1, 1, -1, 1, -1] 0
science, 34.3 [1, -1, -1, 1, -1] 0.916
...

The results above are for the first three webpage urls given in the file 'urls.txt'. The values should be listed in the order of the urls given in 'urls.txt'.

from bs4 import BeautifulSoup 
import urllib 
import re  
import numpy as np
### YOUR CODE HERE 
word_list = ['data', 'companies', 'business', 'action', 'mining', 'science']  
​
urls = open('urls.txt', 'r') 
url_list = [] 
for url in urls: 
    url_list.append(url.strip()) 
​
pgraph_list = [] 
#count = 0 
#stop = open('stop_words.txt', 'r') 
#stop_list = [] 
#for line in stop: 
#   stop_list.append(line.strip().lower())
for url in url_list:   
    #Web parsing 
    req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
    webUrl = urllib.request.urlopen(req).read() 
    soup = BeautifulSoup(webUrl, 'lxml')  
    paragraphs = soup.find_all('p')   
    pgraphs = '' 
    # Extracting paragraphs from webpages
    for p in paragraphs:
        ptext = p.getText().strip().lower() 
        pgraphs += ptext + ' '      
    rec = re.compile(r'\w+|\d+') 
    totalText = pgraphs.strip() 
    words = rec.findall(totalText) 
    wlist = []
    for word in words:
        #if word not in stop_list: 
        wlist.append(word)
#Dumping all word lists from each URL into one master list
    pgraph_list.append(wlist)  
urls.close() 
#stop.close()  
​
combo_list = [] 
comp_list = []
for i in range(0, len(word_list)): 
    combo_list.append([])  
    comp_list.append([])
​
for graph in pgraph_list: 
    for i in range(0, len(word_list)):  
        combo_list[i].append(graph.count(word_list[i])) 
​
avg_list = [] 
for combo in combo_list: 
    temp_count = 0
    for num in combo:  
        temp_count += num 
    avg_list.append(temp_count / len(combo))   
    
for i in range(0, len(combo_list)): 
    for j in combo_list[i]: 
        if(j  > avg_list[i]): 
            comp_list[i].append(1) 
        else: 
            comp_list[i].append(-1)
​
idf_list = [] 
for combo in combo_list: 
    top = len(combo) 
    temp_count = 1
    for num in combo:
        if (num != 0):
            temp_count += 1
    idf_list.append(np.log(top / (temp_count))) 
    
file = open('Q3.txt', 'w')
​
        
for i in range(0, len(word_list)): 
    file.write(str(word_list[i]) + ' ' + str(avg_list[i]) + ' ' + str(comp_list[i]) + ' ' +  str(idf_list[i]) + '\n') #' ' + str(combo_list[i]) + 
​
file.close()
​
file1 = open('Q3.txt', 'r') 
for line in file1:
    print(line.strip()) 
file1.close()    
data 56.166666666666664 [1, 1, -1, 1, -1, -1] 0.0
companies 2.6666666666666665 [-1, -1, -1, -1, 1, -1] 0.1823215567939546
business 7.5 [-1, 1, -1, 1, -1, -1] 0.1823215567939546
action 0.16666666666666666 [1, -1, -1, -1, -1, -1] 1.0986122886681098
mining 1.0 [1, 1, -1, -1, -1, -1] 0.4054651081081644
science 20.166666666666668 [1, 1, -1, -1, 1, -1] -0.15415067982725836
Q4: Bigram Features (10)
A bigram is a sequence of two adjacent words in a string of words. For example, consider the following sentence: "a screaming comes across the sky" The bigrams in this sentence would be:

a screaming
screaming comes
comes across
across the
the sky
For each webpage document, read the content and do the following:

A. Parse all paragraph text, convert to lowercase, strip out punctuation, and tokenize based on whitespace separation as for Q1 (i.e. steps A-D). B. For this question, do not exclude the stop words. C. Compute a list of bigrams for each document, where each bigram is a concatenation of two adjacent words. Example: If the list of words found after tokenizing is ['screaming', 'comes', 'across', 'sky'], then the list of bigrams would be ['screamingcomes', 'comesacross', 'acrosssky']

Find the set of unique bigrams across all the documents and sort them in lexicographic order. This will form the vocabulary of bigrams.
Find the set of bigrams that occur in more than one document.
Output the results in a file named 'Q4.txt' for evaluation.
In the output file, report the number of bigrams found in step 2 in the first line, followed by each bigram (listed in lexicographic order) found in step 3 on a new line.

The format of the output file is given below:

Bigram Size \t < Bigram Size >
bigram1
bigram2
...

For example,

Bigram Size    3410
adata
algorithmsand
...
...
betweendata
...
...
closelywith
...
...

The results above are for the first three webpage urls given in the file 'urls.txt'. (This is only an example output.)

from bs4 import BeautifulSoup 
import urllib 
import re 
​
### YOUR CODE HERE 
urls = open('urls.txt', 'r') 
url_list = [] 
for url in urls: 
    url_list.append(url.strip()) 
​
pgraph_list = [] 
#count = 0 
#stop = open('stop_words.txt', 'r') 
#stop_list = [] 
#for line in stop: 
#   stop_list.append(line.strip().lower())
for url in url_list:   
    #Web parsing 
    req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
    webUrl = urllib.request.urlopen(req).read() 
    soup = BeautifulSoup(webUrl, 'lxml')  
    paragraphs = soup.find_all('p')   
    pgraphs = '' 
    # Extracting paragraphs from webpages
    for p in paragraphs:
        ptext = p.getText().strip().lower() 
        pgraphs += ptext + ' ' 
    rec = re.compile(r'\w+|\d+') 
    totalText = pgraphs.strip() 
    words = rec.findall(totalText) 
    wlist = []
    for word in words:
        #if word not in stop_list: 
        wlist.append(word)
#Dumping all word lists from each URL into one master list
    pgraph_list.append(wlist)  
urls.close()   
​
bComboList = [] 
for i in range(0, len(url_list)): 
    bComboList.append([]) 
​
#print(len(pgraph_list[0])) 
count = 0
for i in range(0, len(pgraph_list)):  
    for j in range(1, len(pgraph_list[i])):   
        bComboList[i].append(str(pgraph_list[i][j-1]) + str(pgraph_list[i][j])) 
       # count += 1 
       # print(count)
        #else: 
         #   bComboList[i].append(str(pgraph_list[j]) + str(pgraph_list[len(pgraph_list) - 1]))
        
​
bigram_dict = {} 
unique_list = []
​
for bCombo in bComboList: 
    for word in bCombo: 
        if word not in bigram_dict: 
            bigram_dict[word] = 1 
            unique_list.append(word) 
        else: 
            bigram_dict[word] +=1  
            
#print(bigram_dict)            
#print(len(unique_list))     
final_list = [] 
for item in bigram_dict:
    if bigram_dict[item] != 1: 
        final_list.append(item) 
​
ffinal_list = sorted(final_list) 
​
file = open('Q4.txt', 'w') 
file.write('Bigram Size' + '   ' + str(len(unique_list)) + '\n') 
for f in ffinal_list: 
    file.write(f + '\n') 
file.close()
file1 = open('Q4.txt', 'r') 
for line in file1: 
    print(line) 
file1.close()    
​
Bigram Size   6384

0http

0via

12

2020purdue

21stcentury

30

3min

40

4941776

765494

abachelor

abilitiesto

abilityto

ableto

aboutstatistics

aboutuncovering

aboutwhat

abusiness

abuzz

accessand

accessequal

accessibilityissue

achievebusiness

achieveits

acohesive

acommon

acompany

actionableinsights

adata

adiagram

adiscussion

adiverse

afew

aifor

aimedat

algorithmsand

allabout

allof

allthe

alongwith

alot

amajor

americanstatistical

analysisand

analystdata

analyticsand

analyticshas

anapplied

and2

anda

andalgorithms

andapis

andbig

andbusiness

andcan

andchanges

andcommunicate

andcommunication

andcreate

anddata

anddeliverables

andhigh

andhow

andingenuity

andis

andit

andlibraries

andmachine

andmany

andmodeling

andnewsql

andor

andother

andothers

andquery

andrelated

andresponsibilities

andso

andsolution

andsometimes

andstaff

andstatistics

andstored

andsubsequent

andtechniques

andtechnologies

andtechnology

andthe

andtheir

andthus

andto

andverbal

andwork

anequal

anew

anexecutive

anexpert

animportant

anorganization

applicationof

applicationsof

applicationswhich

aquantitative

architectureand

area

areasof

aremany

aremost

arenot

arent

areoften

areother

aresome

arevery

arounddata

artificialintelligence

asa

asan

asbeing

asbig

asintegrate

aslew

asmentioned

asneeded

aspecific

aspectof

associatedwith

assuch

asthe

astrong

aswell

ata

atechnical

aterm

atpurdue

atthe

attributedto

atype

availableand

availableto

avariety

bachelors

backto

basedon

bea

beable

bean

becausedata

becomea

becominga

beenthe

beingable

beingused

bestjob

beused

bigdata

blendof

businessacumen

businessdecisions

businessdomain

businessgoals

businessintelligence

businessproblems

businessvalue

butalso

butthere

buzzor

bydata

bysa

bythe

byusing

canbe

canhave

careeroptions

ccby

centerinternational

certificationprograms

checkout

choosethe

classicalstats

closelywith

collegeof

columbiauniversity

comefrom

comein

comingup

commondata

communicationand

companieshave

companys

completethe

computerprogramming

computerscience

computingand

concernedwith

contactthe

copyrightcomplaints

covid19

creativecommonsorg

creativityand

culturalcenter

dataanalysis

dataanalyst

dataanalysts

dataand

dataarchitecture

dataas

databasemanagement

datacan

datadata

datadriven

datae

dataengineer

dataengineers

datafor

datafrom

datahas

datain

datainsight

datainsights

datainto

datais

datalake

datamining

datamunging

dataproduct

datascience

datascientist

datascientists

dataset

datasets

datasources

datastorage

datathat

datathrough

datato

datawhich

decisionsand

dedicatedto

definitionof

deliverthe

departmentof

dependon

developmentof

differentfrom

digitaldata

disabilityrelated

discoveryand

discoveryof

discussionon

diversegroup

diversityin

doesnt

doesthis

domainexpertise

dont

drawupon

drivenby

drivendata

dueto

duringthe

eachof

easilyand

eg

engineersare

entrylevel

equalaccess

equallystrong

equalopportunity

evenif

examplea

exampleof

examplesof

experienceand

experienceto

expertin

factthat

facultyand

featuresvariables

feelfree

field29

fieldsand

fieldthat

findingsto

focusedon

fora

foran

foranalysis

forcomputer

fordata

forexample

forhigh

forpeople

forstatistics

forthe

forus

freeto

froma

fromdata

frommany

frommultiple

fromstatistics

fromthe

gabdoprocess

geta

goaland

goalis

goalof

goalsand

groundedin

groupof

hackeris

hasbeen

havea

havebeen

havedescribed

havethe

haveto

havingthe

hereare

herei

hereis

highdemand

highschool

howdo

howto

httpcreativecommons

humanresources

iam

idealcase

ie

ifyou

imagerecognition

impacton

implementthe

importantand

importantfor

importantpart

improveon

in47907

ina

inaddition

inall

inamerica

inan

includingthe

incomputer

incontrast

increaserevenue

indata

indeveloping

informationand

infrastructureas

inmy

innature

innoarchitechinstitute

inone

inorder

inour

inreality

insightand

insightscan

intellectualcuriosity

intelligenceand

intendedto

interpretationof

intersectionof

inthat

inthe

intheir

inthese

inthis

intothe

inways

isa

isall

isalso

isan

iscritical

ishelpful

isimportant

isin

isintended

isnot

isone

issomewhat

isthe

isto

isvery

itas

itcan

itis

its

itsgoals

ituses

ive

jobin

juliaand

knowledgeand

knowledgeof

knowmore

lafayettein

lastlydata

launchedthe

leadto

learningalgorithms

learningand

lets

licensesby

likelyto

liketo

lotof

lotsof

machinelearning

majordepartment

majorspecific

managementsystems

mans

manydifferent

manymore

manyof

manyother

masters

mathand

mathematicsstatistics

maximizeresults

mayhave

maynot

meaningfulinformation

minvideo

modelthat

moreabout

moresophisticated

mostcommon

muchmore

muchof

multiculturalscience

mustbe

namefor

neededin

needto

newname

newones

nodata

noone

nosqland

nota

notbe

notrequire

ofa

ofadmissions

ofan

ofanalytics

ofany

ofas

ofbig

ofcompanies

ofcomputer

ofdata

ofdifferent

ofexpertise

officeof

offthe

ofinformation

ofmathematics

ofscience

ofstatistics

ofstudents

oftencome

ofthe

ofthese

ofthis

ofunderrepresented

ofwomen

ona

onall

ondata

onecan

oneof

oneor

onesas

onhow

onit

onstatistical

onthat

onthe

onthese

onthis

opportunityuniversity

optionsavailable

ora

orderto

orglicenses

otherhand

outof

outthe

outthis

overthe

ownwork

packagesand

pagedisability

partof

pastseveral

pathto

patternsin

peopleand

performthe

phd

pleasecontact

predictionengine

pricecomparison

problemsand

problemsthis

processand

processmodel

productis

productsare

programminglanguages

programmingskills

programthe

protectpurdue

purchasingpower

purduediversity

purdueedu

purdues

purdueuniversity

pythonr

quantitativedata

quantitativetechnique

querymany

questionsand

quitedifferent

rand

rawdata

rdbmsnosql

reasonwhy

recommendationengine

redshiftsnowflake

referringto

referto

relatedaccessibility

relatedto

relativelyeasily

researchand

responsiblefor

resultsand

resultsto

roleand

rolein

roleis

sa3

sa4

sametime

sciencealgorithms

scienceand

scienceas

sciencehas

sciencein

scienceis

scienceprocess

scienceprograms

scienceprojects

sciencethat

sciencethe

scienceto

sciencewe

sciencewhich

scientificmethod

scientistas

scientisthas

scientistis

scientistmay

scientistrole

scientists

scientistsand

scientistsare

scientistsis

scientistsmay

scientistsmust

scientistsshould

scientiststypically

scientistsusually

scientistthat

scientistto

scientistvenn

scopeof

sdata

sdegree

searchengines

searchresults

secretsauce

sectionon

sensedata

serviceproviders

setis

settingup

severalyears

shouldbe

significantbusiness

similarto

skillsand

skillsas

skillset

skillsto

slewof

smarterbusiness

softwaredata

softwarepackages

solutionsto

solutionto

solveanalytically

someexamples

someof

sometimesit

soon

sothat

sourcesand

specificproblem

speechrecognition

statisticalassociation

statisticallearning

statisticsanalytics

statisticsand

statisticsmathematics

stephankolassa

strongbusiness

strongin

studentsand

studentsfaculty

studentsto

successand

successfulin

suchas

suggestionsin

synonymouswith

systemto

technicalskills

techniquesand

technologiesand

technologyand

technologyin

tella

tellyou

tendto

thata

thatare

thatcan

thatdata

thatis

thatlies

thatmany

thats

thatstatistics

thatthe

thatthere

thatthese

thatthey

thatwe

the21st

theability

theabove

theamerican

thebest

thebusiness

thecompany

thecore

thecorrect

thedata

thedefinition

thedepartment

thefact

thefield

thefirst

thegabdo

theidea

theideal

theimpact

theintent

theintersection

theirfindings

theirmajor

theirown

theirway

themin

themost

themto

thenext

theoffice

theother

thepast

theprimary

theproblem

theprocess

thequestions

thereal

thereare

thereason

thereforenot

thereis

theright

thesame

thescientific

theseare

thesecompanies

theseinclude

thesepillars

thesewebsites

thesewill

theshelf

thesituation

thesolution

theterm

thetools

thetop

thetwo

thetypical

thevideos

theworld

theyare

theydo

theymay

theyneed

theywould

thinkthat

this3

thisarticle

thisdata

thisdefinition

thisdiagram

thisis

thismajor

thismeans

thispage

thisrequires

thissense

thoughtof

toa

toaccess

toachieve

toadd

toanalyze

tobe

tobecome

tobecoming

tobring

tobuild

tocome

tocomplete

todata

todeliver

todescribe

toextract

togenerate

toget

tohave

tohelp

tohelping

toimprove

toknow

tolearn

toleverage

tomake

tomaximize

tomeet

toolsand

toolssuch

topdown

topnotch

topush

tosolve

totell

tothe

tothink

tounderstand

touse

toutilize

transfercriteria

transferstudents

troublewith

typeof

underrepresentedminorities

understandand

understandthe

universitieshave

universityan

universitycopyright

upa

upin

upof

upto

usedas

usedby

usedfor

usedin

usedrelatively

usedto

usein

useof

userexperience

users

usingdata

usingmachine

usingthis

varietyof

venndiagrams

verysimilar

viawikimedia

wantto

wasnt

waysthat

waysto

weare

wehave

weknow

wellas

wellsome

westlafayette

whatdoes

whatthey

whichare

whichcan

whichcustomer

whichi

whichis

whichresembles

whileusing

wikimediacommons

willbe

withbig

withcomplex

withdata

withhigh

withinthe

withthe

withthem

withthis

womenin

womens

workcc

wouldhelp

wouldnt

writesthat

writtenand

youll

youthe

youwould

​

