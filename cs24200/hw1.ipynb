Jupyter NotebookHomework1 Last Checkpoint: 09/14/2020 (autosaved)Python 3 [3.6]
# Homework 1
​
#### Please read the following instructions carefully.
​
__Submit all code as a single Jupyter notebook (a .ipynb file.) All result/output files should be submitted along with the code, with each file named as per the instructions provided in the question.__ 
​
Any paragraph/descriptive/yes-no answers that you provide for grading must be included as __separate markdown cells__ in the Jupyter notebook (rather than as comments in the code - these will _not_ be accepted.) These cells should accompany the associated questions: graders cannot be expected to hunt through your submission to find answers to individual questions. 
​
_Submit all solution files to Blackboard before the deadline: __11:59pm EST on Monday, September 7, 2020__._
​
For this homework, you can __ONLY__ use the python libraries __math__ and __numpy__ (for mathematical functions like log), __urllib__ and __BeautifulSoup__ (to read and parse web pages) and __re__ (for regular expressions). Please post any questions you have on Piazza, following the appropriate netiquette.
​
In this assignment, you will retrieve and parse webpages using BeautifulSoup.
​
Note: For all questions, the words should be converted to lower case.
Q1: Part 1 (4pts)
Write a function freqSixLetterWords() to parse the text enclosed in paragraph tags of a webpage, finding all six letter words and counting the number of times each word appears.

The function should:

Take a URL to the webpage as a parameter.
Return a dictionary of words and their corresponding frequencies
Apply the function on the input URL "https://en.wikipedia.org/wiki/COVID-19_pandemic", storing the result in a file, "Q1_Part1.txt". Each line of the Q1_Part1.txt should have the form: "word,count". The words should first be sorted alphabetically, then in decreasing order of frequency (with the most frequent word appearing at the top and the least frequent word at the bottom.)

Example:

mainly,10
tennis,8
redrum,6
august,2
friday,2
twenty,2

Please note that the content of the aforementioned Wikipedia page changes by the hour, if not more frequently. Therefore, the values given in the example above may not be valid for long. We will determine the actual values at the time of grading. If your code is accurate, your output should match that of the graders'. This also stands for Q1 Part 2 and Q2.

from bs4 import BeautifulSoup
import urllib
import re
​
def freqSixLetterWords(url):
###
### YOUR CODE HERE
###   
    
    # parsing url to data
    webUrl = urllib.request.urlopen(url).read()  
    soup = BeautifulSoup(webUrl, 'html5lib') 
    paragraphs = soup.find_all('p') 
    links = soup.find_all('a')  
    
    pcount = ""  
    # Paragraph texts
    for p in paragraphs:  
        ptext = p.getText().strip().lower() 
        pcount += ptext + ' '
     
    # pattern, finds all alpha numeric strings in pcount
    rec = re.compile(r"\w+|\d+") 
    totalText = pcount.strip()    
    words = rec.findall(totalText) 
    
    # organizing the words in words into a dictionary, in this step I am only filtering out the non 6 letter words. 
    maindict = {}   
    
    for word in words: 
        if word not in maindict:  
            if len(word) == 6:
                maindict[word] = 1 
        else:  
            if len(word) == 6:
                maindict[word] += 1 
    
    #I am reversing/sorting the maindict
    ddict = sorted(maindict.items(), key=lambda x: x[1], reverse = True) 
    
    # I am appending to a list all of the words and occurences into a list
    nlist = []
    for item in ddict: 
        for i in item: 
            new_string = item[0] + ',' + str(item[1])
            if new_string not in nlist: 
                if (len(item[0]) == 6): 
                    nlist.append(new_string) 
   
    
    # Opening a file to read the list into
    file = open('Q1_part1.txt', 'w') 
    
    for pair in nlist: 
        file.write(pair + '\n') 
        
    file.close() 
    
    # more for testing purposes, but am seeing if the file I wrote the data into works. It does. 
    file1 = open('Q1_part1.txt', 'r')
    for line in file1: 
        print(line.strip())
    file1.close()
    
    # not sure what I have to return for this problem so I am just returning the sorted dict. 
    return(ddict)
    
    
print(freqSixLetterWords('https://en.wikipedia.org/wiki/COVID-19_pandemic'))    
​
people,82
health,54
deaths,42
spread,39
number,38
public,31
united,30
social,29
states,28
travel,27
global,20
tested,14
europe,14
person,12
caused,11
events,11
brazil,11
crisis,11
around,10
severe,9
report,9
issued,9
cities,9
during,9
across,8
update,8
higher,8
within,8
impact,8
places,7
closed,7
market,7
likely,7
active,7
france,7
result,7
though,7
groups,7
months,7
centre,6
august,6
should,6
entire,6
police,6
single,6
russia,6
access,6
common,5
period,5
supply,5
origin,5
called,5
region,5
system,5
easily,5
longer,5
online,5
reduce,5
office,5
became,5
demand,5
levels,5
nearly,5
africa,5
saying,5
threat,5
trials,4
traced,4
inside,4
source,4
normal,4
almost,4
before,4
remain,4
tissue,4
stages,4
mobile,4
notice,4
warned,4
second,4
record,4
passed,4
raised,4
season,4
safety,3
buying,3
review,3
having,3
excess,3
volume,3
taking,3
others,3
bodies,3
banned,3
either,3
rather,3
taiwan,3
mostly,3
series,3
behind,3
agency,3
church,3
sweden,3
deemed,3
london,3
signed,3
donald,3
sector,3
visits,3
prices,3
arabia,3
hunger,3
school,3
racism,3
racist,3
asians,3
breath,2
fallen,2
huanan,2
thirds,2
jumped,2
adults,2
weekly,2
ranges,2
listed,2
causes,2
oxford,2
stated,2
making,2
depend,2
extent,2
sputum,2
become,2
appear,2
twelve,2
nature,2
detect,2
twenty,2
toilet,2
immune,2
engage,2
proper,2
minute,2
strict,2
failed,2
limits,2
expert,2
affect,2
fluids,2
poorer,2
center,2
linked,2
george,2
vision,2
doctor,2
border,2
orders,2
except,2
needed,2
friday,2
combat,2
hassan,2
former,2
leaked,2
nation,2
korean,2
showed,2
lowest,2
policy,2
decree,2
sports,2
asking,2
madrid,2
venues,2
placed,2
advice,2
citing,2
income,2
useful,2
motion,2
canada,2
cruise,2
troops,2
unicef,2
posted,2
tedros,2
middle,2
losses,2
blamed,2
delays,2
barrel,2
league,2
appeal,2
famine,2
energy,2
hazard,1
worked,1
counts,1
donors,1
cohort,1
stable,1
ignore,1
versus,1
varies,1
follow,1
cannot,1
differ,1
maciej,1
widely,1
phlegm,1
muscle,1
throat,1
chills,1
sudden,1
waking,1
bluish,1
sepsis,1
septic,1
kidney,1
metres,1
indoor,1
floors,1
amount,1
viable,1
copper,1
bleach,1
saliva,1
direct,1
routes,1
breast,1
genome,1
easier,1
enable,1
ground,1
faster,1
eating,1
killed,1
bursts,1
bubble,1
sticky,1
manner,1
sexual,1
sneeze,1
sodium,1
iodine,1
shared,1
remote,1
spaces,1
remove,1
micron,1
device,1
fevers,1
trying,1
manage,1
alerts,1
growth,1
method,1
phones,1
google,1
create,1
chains,1
unable,1
patent,1
valves,1
oxygen,1
plasma,1
option,1
worsen,1
surged,1
advise,1
injury,1
author,1
animal,1
lavage,1
sample,1
jixian,1
medlab,1
enough,1
helped,1
lancet,1
reopen,1
dalian,1
leader,1
cordon,1
spring,1
regard,1
height,1
inland,1
permit,1
moment,1
phased,1
unlock,1
mumbai,1
nowruz,1
screen,1
annual,1
french,1
macron,1
fourth,1
eleven,1
ethics,1
triage,1
planes,1
german,1
gomera,1
canary,1
swedes,1
capita,1
behalf,1
moving,1
unlike,1
petrol,1
ensure,1
marred,1
halted,1
answer,1
little,1
actual,1
earned,1
helena,1
recent,1
unless,1
aboard,1
indian,1
formal,1
labour,1
rights,1
urging,1
onward,1
robust,1
agreed,1
deeply,1
agathe,1
points,1
ending,1
credit,1
rating,1
stocks,1
absorb,1
retail,1
claims,1
coping,1
street,1
facing,1
senior,1
starve,1
monday,1
uphold,1
funded,1
attend,1
square,1
medina,1
summer,1
themed,1
humour,1
fields,1
manuel,1
member,1
medics,1
dmitry,1
peskov,1
norway,1
ursula,1
ensued,1
strike,1
poland,1
baltic,1
hudson,1
letter,1
anyone,1
coming,1
powers,1
viktor,1
punish,1
turkey,1
locust,1
issues,1
beyond,1
itself,1
attack,1
scared,1
timely,1
marked,1
casual,1
modern,1
coined,1
carbon,1
oxides,1
budget,1
amazon,1
toward,1
family,1
jamaat,1
ethnic,1
papers,1
covert,1
backed,1
scheme,1
effect,1
[('people', 82), ('health', 54), ('deaths', 42), ('spread', 39), ('number', 38), ('public', 31), ('united', 30), ('social', 29), ('states', 28), ('travel', 27), ('global', 20), ('tested', 14), ('europe', 14), ('person', 12), ('caused', 11), ('events', 11), ('brazil', 11), ('crisis', 11), ('around', 10), ('severe', 9), ('report', 9), ('issued', 9), ('cities', 9), ('during', 9), ('across', 8), ('update', 8), ('higher', 8), ('within', 8), ('impact', 8), ('places', 7), ('closed', 7), ('market', 7), ('likely', 7), ('active', 7), ('france', 7), ('result', 7), ('though', 7), ('groups', 7), ('months', 7), ('centre', 6), ('august', 6), ('should', 6), ('entire', 6), ('police', 6), ('single', 6), ('russia', 6), ('access', 6), ('common', 5), ('period', 5), ('supply', 5), ('origin', 5), ('called', 5), ('region', 5), ('system', 5), ('easily', 5), ('longer', 5), ('online', 5), ('reduce', 5), ('office', 5), ('became', 5), ('demand', 5), ('levels', 5), ('nearly', 5), ('africa', 5), ('saying', 5), ('threat', 5), ('trials', 4), ('traced', 4), ('inside', 4), ('source', 4), ('normal', 4), ('almost', 4), ('before', 4), ('remain', 4), ('tissue', 4), ('stages', 4), ('mobile', 4), ('notice', 4), ('warned', 4), ('second', 4), ('record', 4), ('passed', 4), ('raised', 4), ('season', 4), ('safety', 3), ('buying', 3), ('review', 3), ('having', 3), ('excess', 3), ('volume', 3), ('taking', 3), ('others', 3), ('bodies', 3), ('banned', 3), ('either', 3), ('rather', 3), ('taiwan', 3), ('mostly', 3), ('series', 3), ('behind', 3), ('agency', 3), ('church', 3), ('sweden', 3), ('deemed', 3), ('london', 3), ('signed', 3), ('donald', 3), ('sector', 3), ('visits', 3), ('prices', 3), ('arabia', 3), ('hunger', 3), ('school', 3), ('racism', 3), ('racist', 3), ('asians', 3), ('breath', 2), ('fallen', 2), ('huanan', 2), ('thirds', 2), ('jumped', 2), ('adults', 2), ('weekly', 2), ('ranges', 2), ('listed', 2), ('causes', 2), ('oxford', 2), ('stated', 2), ('making', 2), ('depend', 2), ('extent', 2), ('sputum', 2), ('become', 2), ('appear', 2), ('twelve', 2), ('nature', 2), ('detect', 2), ('twenty', 2), ('toilet', 2), ('immune', 2), ('engage', 2), ('proper', 2), ('minute', 2), ('strict', 2), ('failed', 2), ('limits', 2), ('expert', 2), ('affect', 2), ('fluids', 2), ('poorer', 2), ('center', 2), ('linked', 2), ('george', 2), ('vision', 2), ('doctor', 2), ('border', 2), ('orders', 2), ('except', 2), ('needed', 2), ('friday', 2), ('combat', 2), ('hassan', 2), ('former', 2), ('leaked', 2), ('nation', 2), ('korean', 2), ('showed', 2), ('lowest', 2), ('policy', 2), ('decree', 2), ('sports', 2), ('asking', 2), ('madrid', 2), ('venues', 2), ('placed', 2), ('advice', 2), ('citing', 2), ('income', 2), ('useful', 2), ('motion', 2), ('canada', 2), ('cruise', 2), ('troops', 2), ('unicef', 2), ('posted', 2), ('tedros', 2), ('middle', 2), ('losses', 2), ('blamed', 2), ('delays', 2), ('barrel', 2), ('league', 2), ('appeal', 2), ('famine', 2), ('energy', 2), ('hazard', 1), ('worked', 1), ('counts', 1), ('donors', 1), ('cohort', 1), ('stable', 1), ('ignore', 1), ('versus', 1), ('varies', 1), ('follow', 1), ('cannot', 1), ('differ', 1), ('maciej', 1), ('widely', 1), ('phlegm', 1), ('muscle', 1), ('throat', 1), ('chills', 1), ('sudden', 1), ('waking', 1), ('bluish', 1), ('sepsis', 1), ('septic', 1), ('kidney', 1), ('metres', 1), ('indoor', 1), ('floors', 1), ('amount', 1), ('viable', 1), ('copper', 1), ('bleach', 1), ('saliva', 1), ('direct', 1), ('routes', 1), ('breast', 1), ('genome', 1), ('easier', 1), ('enable', 1), ('ground', 1), ('faster', 1), ('eating', 1), ('killed', 1), ('bursts', 1), ('bubble', 1), ('sticky', 1), ('manner', 1), ('sexual', 1), ('sneeze', 1), ('sodium', 1), ('iodine', 1), ('shared', 1), ('remote', 1), ('spaces', 1), ('remove', 1), ('micron', 1), ('device', 1), ('fevers', 1), ('trying', 1), ('manage', 1), ('alerts', 1), ('growth', 1), ('method', 1), ('phones', 1), ('google', 1), ('create', 1), ('chains', 1), ('unable', 1), ('patent', 1), ('valves', 1), ('oxygen', 1), ('plasma', 1), ('option', 1), ('worsen', 1), ('surged', 1), ('advise', 1), ('injury', 1), ('author', 1), ('animal', 1), ('lavage', 1), ('sample', 1), ('jixian', 1), ('medlab', 1), ('enough', 1), ('helped', 1), ('lancet', 1), ('reopen', 1), ('dalian', 1), ('leader', 1), ('cordon', 1), ('spring', 1), ('regard', 1), ('height', 1), ('inland', 1), ('permit', 1), ('moment', 1), ('phased', 1), ('unlock', 1), ('mumbai', 1), ('nowruz', 1), ('screen', 1), ('annual', 1), ('french', 1), ('macron', 1), ('fourth', 1), ('eleven', 1), ('ethics', 1), ('triage', 1), ('planes', 1), ('german', 1), ('gomera', 1), ('canary', 1), ('swedes', 1), ('capita', 1), ('behalf', 1), ('moving', 1), ('unlike', 1), ('petrol', 1), ('ensure', 1), ('marred', 1), ('halted', 1), ('answer', 1), ('little', 1), ('actual', 1), ('earned', 1), ('helena', 1), ('recent', 1), ('unless', 1), ('aboard', 1), ('indian', 1), ('formal', 1), ('labour', 1), ('rights', 1), ('urging', 1), ('onward', 1), ('robust', 1), ('agreed', 1), ('deeply', 1), ('agathe', 1), ('points', 1), ('ending', 1), ('credit', 1), ('rating', 1), ('stocks', 1), ('absorb', 1), ('retail', 1), ('claims', 1), ('coping', 1), ('street', 1), ('facing', 1), ('senior', 1), ('starve', 1), ('monday', 1), ('uphold', 1), ('funded', 1), ('attend', 1), ('square', 1), ('medina', 1), ('summer', 1), ('themed', 1), ('humour', 1), ('fields', 1), ('manuel', 1), ('member', 1), ('medics', 1), ('dmitry', 1), ('peskov', 1), ('norway', 1), ('ursula', 1), ('ensued', 1), ('strike', 1), ('poland', 1), ('baltic', 1), ('hudson', 1), ('letter', 1), ('anyone', 1), ('coming', 1), ('powers', 1), ('viktor', 1), ('punish', 1), ('turkey', 1), ('locust', 1), ('issues', 1), ('beyond', 1), ('itself', 1), ('attack', 1), ('scared', 1), ('timely', 1), ('marked', 1), ('casual', 1), ('modern', 1), ('coined', 1), ('carbon', 1), ('oxides', 1), ('budget', 1), ('amazon', 1), ('toward', 1), ('family', 1), ('jamaat', 1), ('ethnic', 1), ('papers', 1), ('covert', 1), ('backed', 1), ('scheme', 1), ('effect', 1)]
###
### AUTOGRADER TEST - DO NOT REMOVE
###
​
###
### AUTOGRADER TEST - DO NOT REMOVE
###
​
Q1: Part 2 (4pts)
Stop words are natural language words which have very little meaning, such as "and", "the", "a", "also", etc. The file "stop_words.txt" contains a list of stop words (this file is provided in the same directory as this notebook.)

Repeat Part 1 for the function freqSixLetterWordsNoStopWords(), but this time, remove all stop words given in the file "stop_words.txt" before counting. The output for Part 2 should have the same format as Part 1, should be applied on the same input URL "https://en.wikipedia.org/wiki/COVID-19_pandemic" as Part 1, and should be written to an output file named "Q1_Part2.txt".

from bs4 import BeautifulSoup
import urllib
import re
​
def freqSixLetterWordsNoStopWords(url):
###
### YOUR CODE HERE
### 
    # opening stop words file and reading them into a list to refer back to
    stopfile = open('stop_words.txt', 'r') 
    stoplist = [] 
    for line in stopfile: 
        stoplist.append(line.strip()) 
    
    stopfile.close() 
    
    # parsing the url into a file
    webUrl = urllib.request.urlopen(url).read()  
    soup = BeautifulSoup(webUrl, 'lxml') 
    paragraphs = soup.find_all('p')  
    
    pcount = ""  
    # Paragraph texts
    for p in paragraphs:  
        ptext = p.getText().strip().lower() 
        pcount += ptext + ' '  
    
    rec = re.compile(r"\w+|\d+") 
    totalText = pcount.strip()    
    words = rec.findall(totalText) 
    
    maindict = {}   
    
    # Within this for loop, not only am I filtering out the non 6 letter words from the list
    # But additionally I am filtering out the stop words
    for word in words: 
        if word not in maindict:  
            if word not in stoplist:
                if len(word) == 6: 
                    maindict[word] = 1 
        else:  
            if word not in stoplist:
                if len(word) == 6:
                    maindict[word] += 1   
    
    
    # flipping censored dict
    ddict = sorted(maindict.items(), key=lambda x: x[1], reverse = True) 
    
    nlist = []
    for item in ddict: 
        for i in item: 
            new_string = item[0] + ',' + str(item[1])
            if new_string not in nlist: 
                if (len(item[0]) == 6): 
                    nlist.append(new_string) 
   
    #print(nlist)
    
    file = open('Q1_part2.txt', 'w') 
    
    for pair in nlist: 
        file.write(pair + '\n') 
    
    file.close()
    
    file1 = open('Q1_part2.txt', 'r')
    for line in file1: 
        print(line.strip())
    file1.close() 
    
    return(ddict)
    
     
​
​
​
print(freqSixLetterWordsNoStopWords('https://en.wikipedia.org/wiki/COVID-19_pandemic'))
     
    
        
    
​
people,82
health,54
deaths,42
spread,39
number,38
public,31
united,30
social,29
states,28
travel,27
global,20
europe,15
tested,14
person,12
caused,11
events,11
brazil,11
crisis,11
severe,9
report,9
issued,9
cities,9
update,8
higher,8
impact,8
places,7
closed,7
market,7
likely,7
active,7
france,7
result,7
groups,7
months,7
online,6
centre,6
august,6
entire,6
police,6
single,6
russia,6
access,6
common,5
period,5
supply,5
origin,5
called,5
region,5
easily,5
longer,5
reduce,5
office,5
demand,5
levels,5
nearly,5
africa,5
saying,5
threat,5
trials,4
traced,4
inside,4
source,4
normal,4
remain,4
tissue,4
stages,4
mobile,4
notice,4
warned,4
second,4
record,4
passed,4
raised,4
season,4
safety,3
buying,3
review,3
having,3
excess,3
volume,3
taking,3
bodies,3
banned,3
taiwan,3
series,3
agency,3
church,3
sweden,3
deemed,3
london,3
signed,3
donald,3
sector,3
visits,3
prices,3
arabia,3
hunger,3
school,3
racism,3
racist,3
asians,3
breath,2
fallen,2
huanan,2
thirds,2
jumped,2
adults,2
weekly,2
ranges,2
listed,2
causes,2
oxford,2
stated,2
making,2
depend,2
extent,2
sputum,2
appear,2
nature,2
detect,2
toilet,2
immune,2
engage,2
proper,2
minute,2
strict,2
failed,2
limits,2
expert,2
affect,2
fluids,2
poorer,2
center,2
linked,2
george,2
vision,2
doctor,2
border,2
orders,2
needed,2
friday,2
combat,2
hassan,2
leaked,2
nation,2
korean,2
showed,2
lowest,2
policy,2
decree,2
sports,2
asking,2
madrid,2
venues,2
placed,2
advice,2
citing,2
income,2
useful,2
motion,2
canada,2
cruise,2
troops,2
unicef,2
posted,2
tedros,2
middle,2
losses,2
blamed,2
delays,2
barrel,2
league,2
appeal,2
famine,2
energy,2
hazard,1
worked,1
counts,1
donors,1
cohort,1
stable,1
ignore,1
versus,1
varies,1
follow,1
differ,1
maciej,1
widely,1
phlegm,1
muscle,1
throat,1
chills,1
sudden,1
waking,1
bluish,1
sepsis,1
septic,1
kidney,1
metres,1
indoor,1
floors,1
viable,1
copper,1
bleach,1
saliva,1
direct,1
routes,1
breast,1
genome,1
easier,1
enable,1
ground,1
faster,1
eating,1
killed,1
bursts,1
bubble,1
sticky,1
manner,1
sexual,1
sneeze,1
sodium,1
iodine,1
shared,1
remote,1
spaces,1
remove,1
micron,1
device,1
fevers,1
trying,1
manage,1
alerts,1
growth,1
method,1
phones,1
google,1
create,1
chains,1
unable,1
patent,1
valves,1
oxygen,1
plasma,1
option,1
worsen,1
surged,1
advise,1
injury,1
author,1
animal,1
lavage,1
sample,1
jixian,1
medlab,1
helped,1
lancet,1
reopen,1
dalian,1
leader,1
cordon,1
spring,1
regard,1
height,1
inland,1
permit,1
moment,1
phased,1
unlock,1
mumbai,1
nowruz,1
screen,1
annual,1
french,1
macron,1
fourth,1
ethics,1
triage,1
planes,1
german,1
gomera,1
canary,1
swedes,1
capita,1
behalf,1
moving,1
unlike,1
petrol,1
ensure,1
marred,1
halted,1
answer,1
little,1
actual,1
earned,1
helena,1
recent,1
unless,1
aboard,1
indian,1
formal,1
labour,1
rights,1
urging,1
onward,1
robust,1
agreed,1
deeply,1
agathe,1
points,1
ending,1
credit,1
rating,1
stocks,1
absorb,1
retail,1
claims,1
coping,1
street,1
facing,1
senior,1
starve,1
monday,1
uphold,1
funded,1
attend,1
square,1
medina,1
summer,1
themed,1
humour,1
fields,1
manuel,1
member,1
medics,1
dmitry,1
peskov,1
norway,1
ursula,1
ensued,1
strike,1
poland,1
baltic,1
hudson,1
letter,1
coming,1
powers,1
viktor,1
punish,1
turkey,1
locust,1
issues,1
attack,1
scared,1
timely,1
marked,1
casual,1
modern,1
coined,1
carbon,1
oxides,1
budget,1
amazon,1
family,1
jamaat,1
ethnic,1
papers,1
covert,1
backed,1
scheme,1
effect,1
[('people', 82), ('health', 54), ('deaths', 42), ('spread', 39), ('number', 38), ('public', 31), ('united', 30), ('social', 29), ('states', 28), ('travel', 27), ('global', 20), ('europe', 15), ('tested', 14), ('person', 12), ('caused', 11), ('events', 11), ('brazil', 11), ('crisis', 11), ('severe', 9), ('report', 9), ('issued', 9), ('cities', 9), ('update', 8), ('higher', 8), ('impact', 8), ('places', 7), ('closed', 7), ('market', 7), ('likely', 7), ('active', 7), ('france', 7), ('result', 7), ('groups', 7), ('months', 7), ('online', 6), ('centre', 6), ('august', 6), ('entire', 6), ('police', 6), ('single', 6), ('russia', 6), ('access', 6), ('common', 5), ('period', 5), ('supply', 5), ('origin', 5), ('called', 5), ('region', 5), ('easily', 5), ('longer', 5), ('reduce', 5), ('office', 5), ('demand', 5), ('levels', 5), ('nearly', 5), ('africa', 5), ('saying', 5), ('threat', 5), ('trials', 4), ('traced', 4), ('inside', 4), ('source', 4), ('normal', 4), ('remain', 4), ('tissue', 4), ('stages', 4), ('mobile', 4), ('notice', 4), ('warned', 4), ('second', 4), ('record', 4), ('passed', 4), ('raised', 4), ('season', 4), ('safety', 3), ('buying', 3), ('review', 3), ('having', 3), ('excess', 3), ('volume', 3), ('taking', 3), ('bodies', 3), ('banned', 3), ('taiwan', 3), ('series', 3), ('agency', 3), ('church', 3), ('sweden', 3), ('deemed', 3), ('london', 3), ('signed', 3), ('donald', 3), ('sector', 3), ('visits', 3), ('prices', 3), ('arabia', 3), ('hunger', 3), ('school', 3), ('racism', 3), ('racist', 3), ('asians', 3), ('breath', 2), ('fallen', 2), ('huanan', 2), ('thirds', 2), ('jumped', 2), ('adults', 2), ('weekly', 2), ('ranges', 2), ('listed', 2), ('causes', 2), ('oxford', 2), ('stated', 2), ('making', 2), ('depend', 2), ('extent', 2), ('sputum', 2), ('appear', 2), ('nature', 2), ('detect', 2), ('toilet', 2), ('immune', 2), ('engage', 2), ('proper', 2), ('minute', 2), ('strict', 2), ('failed', 2), ('limits', 2), ('expert', 2), ('affect', 2), ('fluids', 2), ('poorer', 2), ('center', 2), ('linked', 2), ('george', 2), ('vision', 2), ('doctor', 2), ('border', 2), ('orders', 2), ('needed', 2), ('friday', 2), ('combat', 2), ('hassan', 2), ('leaked', 2), ('nation', 2), ('korean', 2), ('showed', 2), ('lowest', 2), ('policy', 2), ('decree', 2), ('sports', 2), ('asking', 2), ('madrid', 2), ('venues', 2), ('placed', 2), ('advice', 2), ('citing', 2), ('income', 2), ('useful', 2), ('motion', 2), ('canada', 2), ('cruise', 2), ('troops', 2), ('unicef', 2), ('posted', 2), ('tedros', 2), ('middle', 2), ('losses', 2), ('blamed', 2), ('delays', 2), ('barrel', 2), ('league', 2), ('appeal', 2), ('famine', 2), ('energy', 2), ('hazard', 1), ('worked', 1), ('counts', 1), ('donors', 1), ('cohort', 1), ('stable', 1), ('ignore', 1), ('versus', 1), ('varies', 1), ('follow', 1), ('differ', 1), ('maciej', 1), ('widely', 1), ('phlegm', 1), ('muscle', 1), ('throat', 1), ('chills', 1), ('sudden', 1), ('waking', 1), ('bluish', 1), ('sepsis', 1), ('septic', 1), ('kidney', 1), ('metres', 1), ('indoor', 1), ('floors', 1), ('viable', 1), ('copper', 1), ('bleach', 1), ('saliva', 1), ('direct', 1), ('routes', 1), ('breast', 1), ('genome', 1), ('easier', 1), ('enable', 1), ('ground', 1), ('faster', 1), ('eating', 1), ('killed', 1), ('bursts', 1), ('bubble', 1), ('sticky', 1), ('manner', 1), ('sexual', 1), ('sneeze', 1), ('sodium', 1), ('iodine', 1), ('shared', 1), ('remote', 1), ('spaces', 1), ('remove', 1), ('micron', 1), ('device', 1), ('fevers', 1), ('trying', 1), ('manage', 1), ('alerts', 1), ('growth', 1), ('method', 1), ('phones', 1), ('google', 1), ('create', 1), ('chains', 1), ('unable', 1), ('patent', 1), ('valves', 1), ('oxygen', 1), ('plasma', 1), ('option', 1), ('worsen', 1), ('surged', 1), ('advise', 1), ('injury', 1), ('author', 1), ('animal', 1), ('lavage', 1), ('sample', 1), ('jixian', 1), ('medlab', 1), ('helped', 1), ('lancet', 1), ('reopen', 1), ('dalian', 1), ('leader', 1), ('cordon', 1), ('spring', 1), ('regard', 1), ('height', 1), ('inland', 1), ('permit', 1), ('moment', 1), ('phased', 1), ('unlock', 1), ('mumbai', 1), ('nowruz', 1), ('screen', 1), ('annual', 1), ('french', 1), ('macron', 1), ('fourth', 1), ('ethics', 1), ('triage', 1), ('planes', 1), ('german', 1), ('gomera', 1), ('canary', 1), ('swedes', 1), ('capita', 1), ('behalf', 1), ('moving', 1), ('unlike', 1), ('petrol', 1), ('ensure', 1), ('marred', 1), ('halted', 1), ('answer', 1), ('little', 1), ('actual', 1), ('earned', 1), ('helena', 1), ('recent', 1), ('unless', 1), ('aboard', 1), ('indian', 1), ('formal', 1), ('labour', 1), ('rights', 1), ('urging', 1), ('onward', 1), ('robust', 1), ('agreed', 1), ('deeply', 1), ('agathe', 1), ('points', 1), ('ending', 1), ('credit', 1), ('rating', 1), ('stocks', 1), ('absorb', 1), ('retail', 1), ('claims', 1), ('coping', 1), ('street', 1), ('facing', 1), ('senior', 1), ('starve', 1), ('monday', 1), ('uphold', 1), ('funded', 1), ('attend', 1), ('square', 1), ('medina', 1), ('summer', 1), ('themed', 1), ('humour', 1), ('fields', 1), ('manuel', 1), ('member', 1), ('medics', 1), ('dmitry', 1), ('peskov', 1), ('norway', 1), ('ursula', 1), ('ensued', 1), ('strike', 1), ('poland', 1), ('baltic', 1), ('hudson', 1), ('letter', 1), ('coming', 1), ('powers', 1), ('viktor', 1), ('punish', 1), ('turkey', 1), ('locust', 1), ('issues', 1), ('attack', 1), ('scared', 1), ('timely', 1), ('marked', 1), ('casual', 1), ('modern', 1), ('coined', 1), ('carbon', 1), ('oxides', 1), ('budget', 1), ('amazon', 1), ('family', 1), ('jamaat', 1), ('ethnic', 1), ('papers', 1), ('covert', 1), ('backed', 1), ('scheme', 1), ('effect', 1)]
###
### AUTOGRADER TEST - DO NOT REMOVE
###
​
###
### AUTOGRADER TEST - DO NOT REMOVE
###
​
Q2 (8pts)
Write a function linkTexts() that takes a URL as input and finds and counts all outgoing links to other webpages. Again, apply the function on the input URL "https://en.wikipedia.org/wiki/COVID-19_pandemic" and write the output to a file named "Q2.txt", with each outgoing URL written on a new line. Following the URL, denote each UNIQUE string of text used by the link within the document (indented with a tab, '\t'). Denote links with no text using the empty string.

Example: A very simple page with 4 links (http://google.com and http://purdue.edu each twice) may look like this, if both Purdue University links use the same string as text:

http://google.com
    Google
    That one search engine
http://purdue.edu
    Purdue University

from bs4 import BeautifulSoup
import urllib
import re
​
def linkTexts(url):
###
### YOUR CODE HERE
###  
    webUrl = urllib.request.urlopen(url).read()  
    soup = BeautifulSoup(webUrl, 'html5lib') 
    llist = []  
    tlist = [] 
    # I am finding the links and appending them to a list
    for link in soup.findAll('a', attrs={'href': re.compile("^http://")}): 
        llist.append(link.get('href'))
    
    # I am appending 
    tagdict = {}
    for link in soup.findAll('a', attrs={'href': re.compile("^http://")}): 
        lurl = link.get('href') 
        surl = str(lurl)
        if 'http' in surl: 
            linkText = link.get_text().strip() 
            if surl in tagdict: 
                previous = tagdict[surl] 
                tagdict[surl] = previous.append(linkText)
            tagdict[surl] = linkText
    
    fileList = []
    for item in tagdict: 
        x = item + ' ' + str(tagdict[item]) + '\n' 
        fileList.append(x) 
    
    #print(str(len(fileList)) + '$$$$$$$$$$$$$')
    file = open('Q2.txt', 'w') 
    for line in fileList: 
        file.write(line)
     
    file.close() 
    file1 = open('Q2.txt', 'r') 
    for line in file1: 
        print(line)  
    
    file1.close() 
    
    return('')
        
print(linkTexts('https://en.wikipedia.org/wiki/COVID-19_pandemic')) 
​
http://www.imperial.ac.uk/medicine/departments/school-public-health/infectious-disease-epidemiology/mrc-global-infectious-disease-analysis/covid-19/report-13-europe-npi-impact/ "Report 13—Estimating the number of infections and the impact of non-pharmaceutical interventions on COVID-19 in 11 European countries"

http://www.who.int/docs/default-source/coronaviruse/situation-reports/20200219-sitrep-30-covid-19.pdf?sfvrsn=6e50645_2 "WHO COVID-19 situation report 29"

http://www.euro.who.int/en/health-topics/health-emergencies/coronavirus-covid-19/novel-coronavirus-2019-ncov-technical-guidance/coronavirus-disease-covid-19-outbreak-technical-guidance-europe/hospital-readiness-checklist-for-covid-19 "Hospital readiness checklist for COVID-19"

http://www.niaid.nih.gov/news-events/nih-clinical-trial-shows-remdesivir-accelerates-recovery-advanced-covid-19 "NIH Clinical Trial Shows Remdesivir Accelerates Recovery from Advanced COVID-19"

http://china.caixin.com/2020-02-26/101520972.html the original

http://wjw.wuhan.gov.cn/front/web/showDetail/2019123108989 the original

http://weekly.chinacdc.cn/en/article/id/e53946e2-c6c4-41e9-9a9b-fea8db1a8f51 "The Epidemiological Characteristics of an Outbreak of 2019 Novel Coronavirus Diseases (COVID-19)—China, 2020"

http://www.caixin.com/2020-06-12/101566522.html "北京连续确诊3例新冠患者 新发地批发市场暂停营业"

http://www.xinhuanet.com/english/2020-02/10/c_138771533.htm "Xi stresses winning people's war against novel coronavirus"

http://www.xinhuanet.com/english/2020-03/10/c_138863160.htm "All 16 temporary hospitals in Wuhan closed"

http://www.bjnews.com.cn/news/2020/01/24/678863.html 湖北这些学校推迟开学 北大等暂停参观

http://sz.people.com.cn/n2/2020/0125/c202846-33743688.html 深圳：高三初三也不得提前开学提前补课

http://www.ecns.cn/news/2020-01-29/detail-ifzsyram6093862.shtml "China opens more online exhibitions amid virus outbreak"

http://english.scio.gov.cn/m/topnews/2020-03/24/content_75852078.htm "China deploys measures to curb imported COVID-19 cases, rebound in indigenous cases"

http://irangov.ir/detail/334964 "Coronavirus Arrives in Iran: Two People Test Positive in Qom"

http://irangov.ir/detail/335028 "Iran Confirms 3 New Coronavirus Cases"

http://www.koreabiomed.com/news/articleView.html?idxno=7489 "COVID-19 patients soar to 204 in Korea"

http://www.mohw.go.kr/react/al/sal0301vw.jsp?PAR_MENU_ID=04&MENU_ID=0403&page=1&CONT_SEQ=353047&SEARCHKEY=TITLE&SEARCHVALUE=코로나바이러 코로나바이러스감염증-19 국내 발생 현황 (2월 22일 09시)

http://www.koreaherald.com/view.php?ud=20200221000249 "Airlines to suspend more flights over coronavirus"

http://www.koreaherald.com/view.php?ud=20200221000554 "Foreign artists delay concerts in Korea due to spread of COVID-19"

http://www.siaarti.it/SiteAssets/News/COVID19%20-%20documenti%20SIAARTI/SIAARTI%20-%20Covid19%20-%20Raccomandazioni%20di%20etica%20clinica.pdf Raccomandazioni di etica clinica per l'ammissione a trattamenti intensivi e per la loro sospensione, in condizioni eccezionali di squilibrio tra necessità e risorse disponibili

http://www.protezionecivile.gov.it/media-comunicazione/comunicati-stampa/dettaglio/-/asset_publisher/default/content/coronavirus-sono-33-190-i-positivi "Coronavirus: sono 33.190 i positivi—Comunicato Stampa"

http://www.sciencedirect.com/science/article/pii/S1684118220300736 "Internationally lost COVID-19 cases"

http://tempo.com.ph/2020/01/29/ph-sending-special-flights-to-get-pinoys-from-wuhan-hubei-in-china/ "PH sending special flights to get Pinoys from Wuhan, Hubei in China"

http://www.theartnewspaper.com/news/here-are-the-museums-that-have-closed-due-to-coronavirus "Here are the museums that have closed (so far) due to coronavirus"

http://mcn.edu/a-guide-to-virtual-museum-resources/ "The Ultimate Guide to Virtual Museum Resources"

http://www.xinhuanet.com/english/2020-06/07/c_139120863.htm "China CDC informs U.S. of COVID-19 on Jan. 4: white paper"

http://www.thetelegram.com/news/world/coronavirus-deprives-nearly-300-million-students-of-their-schooling-unesco-419714/ "Coronavirus deprives nearly 300 million students of their schooling: UNESCO"

http://www.g-feed.com/2020/03/covid-19-reduces-economic-activity.html "COVID-19 reduces economic activity, which reduces pollution, which saves lives"

http://www.koreatimes.co.kr/www/world/2020/02/683_282767.html "Fears of new virus trigger anti-China sentiment worldwide"

http://www.who.int/news-room/q-a-detail/q-a-coronaviruses Questions & Answers

http://en.nhc.gov.cn/antivirusfight.html COVID-19

http://www.ecdc.europa.eu/en/novel-coronavirus-china/questions-answers Q&A

http://www.moh.gov.sg/covid-19/faqs Q&A

http://unwfp.maps.arcgis.com/apps/opsdashboard/index.html#/42b8837bb25049b9b1f69a9555d55808 World Travel Restrictions


###
### AUTOGRADER TEST - DO NOT REMOVE
###
​
###
### AUTOGRADER TEST - DO NOT REMOVE
###
​
Q3: Part 1 (8pts)
Retrieve and parse multiple web pages. The text file "urls.txt" contains a list of webpages to be parsed. Each line in the text file corresponds to a URL. Use BeautifulSoup to fetch each webpage and parse it as specified below.

For each webpage document do the following:
Retrieve all text enclosed in paragraph tags.
Convert the text to lowercase.
Strip out punctuation: use regular expressions involving \W to replace all sequences of non alpha-numeric characters with a whitespace.
Tokenize into words based on whitespace separation.
Find the number of unique words in each webpage document.
Find the length of each webpage document. The length of a document is defined as the total number of words in the document (not just unique words).
For each of the following words: “coronavirus”, “mask”, and “health”:

Find Term Frequency (tf). The term frequency (tf) of a term (word) is defined as the number of times that term t occurs in document d, divided by the total number of words in the document. The tf of a word depends on the document under consideration.

Find Inverse Document Frequency (idf). The inverse document frequency of a word is the logarithmically scaled inverse fraction of the documents that contain the word, obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that ratio. The idf of a word doesn't depend on any document in which the word is present. To calculate the idf, you will have to use the natural log function. (The base of the natural log function is e.)

Find tf-idf. The tf-idf of a word is the product of the term frequency of the word in document d, and its inverse document frequency. The tf-idf of a word depends on the document under consideration.

Reference: https://en.wikipedia.org/wiki/Tf%E2%80%93idf

The output should be written to an output file named "Q3_Part1.txt".

The format of the output file is as shown below (include the labels on each line):

unique: [3510, 76]
length: [17578, 161]
tf coronavirus: [0.0022755717373990213, 0.0]
tf mask: [0.00022755717373990216, 0.0]
tf health: [0.0029013539651837525, 0.049689440993788817]
idf coronavirus: 0.6931471805599453
idf mask: 0.6931471805599453
idf health: 0.0
tf-idf coronavirus: [0.0015773061339400278, 0.0]
tf-idf mask: [0.0001577306133940028, 0.0]
tf-idf health: [0.0, 0.0]

The above values are for the first two webpage urls given in the file "urls.txt". The number of unique words in documents, average length of documents, tf and tf-idf values for the three words must be in the order of the urls given in "urls.txt". Again, these values might no longer be valid at the time of running your code, so they should be treated as a rough guideline only

You should write a function Q3_P1() which takes no arguments and generates the file "Q3_Part1.txt" containing all the relevant information. You can choose to write any helper routines in accordance with your design, and your Q3_P1() function may call them in any order.

from bs4 import BeautifulSoup 
import urllib 
import re 
import numpy as np 
import math 
​
def Q3_P1(): 
​
    file = open('urls.txt', 'r')  
​
    urlList = []    
​
### Lists to return ###
    uniqueList = [] 
    lengthList = [] 
    tfCovidList = [] 
    tfMaskList = [] 
    tfHealthList = [] 
    idfCovid = 0
    idfMask = 0 
    idfHealth = 0
    tfidfCovidList = [] 
    tfidfMaskList = [] 
    tfidfHealthList = [] 
###---------------##### 
​
#parsing the urls
    for url in file: 
        urlList.append(url.strip())  
        
# for each url I am interating through them
    for url in urlList:   
        # Web parsing
        webUrl = urllib.request.urlopen(url).read()  
        soup = BeautifulSoup(webUrl, 'lxml') 
        paragraphs = soup.find_all('p')   
        pgraphs = '' 
        # getting paragraph texts
        for p in paragraphs:  
            ptext = p.getText().strip().lower() 
            pgraphs += ptext + ' '  
# Setting up my pattern
        rec = re.compile(r"\w+|\d+") 
        totalText = pgraphs.strip()    
        words = rec.findall(totalText) 
        wdict = {}  
        # iterating through the words, to place them in a dictionary
        for word in words: 
            if word not in wdict: 
                wdict[word] = 1 
            else: 
                wdict[word] += 1  
        # Sorting the dictionary        
        ddict = sorted(wdict.items(), key=lambda x: x[1])  
        ### Here I am testing to find the quantities on set {'coronavirus', 'mask', 'health}
        uniquewords = 0   
        covidCount = 0 
        healthCount = 0 
        maskCount = 0
        for tup in ddict:  
            for item in tup:
                if tup[1] == 1: 
                    uniquewords += 1  
                if tup[0] == 'coronavirus': 
                    covidCount += 1 
                if tup[0] == 'mask': 
                    maskCount += 1 
                if tup[0] == 'health': 
                    healthCount += 1 
​
        uniqueList.append(uniquewords)   
        lengthList.append(len(words))  
    
# # #
# # # the words I'm choosing to analyze for tf, idf, and tf-idf are {'coronavirus', 'mask', 'health'}  
# # #     
        tfCovid = covidCount / len(words)  
        tfCovidList.append(tfCovid) 
        tfMask = maskCount / len(words) 
        tfMaskList.append(tfMask) 
        tfHealth = healthCount/len(words)
        tfHealthList.append(tfHealth) 
​
        count1 = 0 
        for num in tfCovidList: 
            if (num != 0.0): 
                count1 += 1
        count2 = 0 
        for num in tfMaskList: 
            if (num != 0.0): 
                count2 += 1  
        count3 = 0 
        for num in tfHealthList: 
            if (num != 0.0): 
                count3 +=1
​
​
    a = 'unique: ' + '' + str(uniqueList)  
    b = 'length: ' + '' + str(lengthList) 
    c = 'tf coronavirus: ' + '' + str(tfCovidList)  
    d = 'tf mask: ' + '' + str(tfMaskList) 
    e = 'tf health: ' + '' + str(tfHealthList) 
    idfCovid = np.log((len(urlList) / (1 + count1))) 
​
    idfMask = np.log(len(urlList) / (1 + count2)) 
​
    idfHealth = np.log(len(urlList) / (1 + count3))  
    
    f = 'idf coronavirus: ' + str(idfCovid)
    g = 'idf mask: ' + str(idfMask)  
    h = 'idf health: ' + str(idfHealth)
​
    for num in tfCovidList: 
        tfidfCovidList.append(idfCovid * num) 
​
    for num in tfMaskList: 
        tfidfMaskList.append(idfMask * num) 
​
    for num in tfHealthList: 
        tfidfHealthList.append(idfHealth * num) 
​
    i = 'tf-idf coronavirus: ' + str(tfidfCovidList) 
    j = 'tf-idf mask: ' + str(tfidfMaskList)
    k = 'tf-idf health: ' + str(tfidfHealthList)
​
    q3file = open('Q3_part1.txt', 'w')  
    lineas = [a,b,c,d,e,f,g,h,i,j,k] 
    for line in lineas: 
        q3file.write(line + '\n') 
     
    q3file.close() 
    
    q3file_1 = open('Q3_part1.txt', 'r') 
    for line in q3file_1: 
        print(line.strip()) 
    
    q3file_1.close() 
    return ('')
    
​
print(Q3_P1())  
​
​
unique: [720, 944, 406, 750, 1212, 376, 844, 664]
length: [1106, 1950, 591, 1517, 2525, 560, 1479, 1154]
tf coronavirus: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
tf mask: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
tf health: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001352265043948614, 0.0]
idf coronavirus: 2.0794415416798357
idf mask: 2.0794415416798357
idf health: 1.3862943611198906
tf-idf coronavirus: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
tf-idf mask: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
tf-idf health: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0018746374051655044, 0.0]

###
### AUTOGRADER TEST - DO NOT REMOVE
###
​
Q3: Part 2 (8pts)
Repeat Part 1, but first remove the stop words given in the file "stop_words.txt".

The output for Part 2 should have the same format as Part 1, and should be written to an output file named "Q3_Part2.txt".

Note: The length of document, in this case, will not include stop words. Similarly, the number of unique words in documents, and the calculation of tf, idf and tf-idf should be done after removing the stop words.

Sample output for the file "urls.txt"

unique: [3279, 56]
words: [8969, 103]
tf coronavirus: [0.004348310848478091, 0.0]
tf mask: [0.0004459805998439068, 0.0]
tf health: [0.005686252648009812, 0.07766990291262135]
idf coronavirus: 0.6931471805599453
idf mask: 0.6931471805599453
idf health: 0.0
tf-idf coronavirus: [0.003014019404820812, 0.0]
tf-idf mask: [0.0003091301953662372, 0.0]
tf-idf health: [0.0, 0.0]

The above values are for the first two webpage urls given in the file "urls.txt". The number of unique words in documents, average length of documents, tf and tf-idf values for the four words, must be in the order of the urls given in "urls.txt". The sample output should only be used for reference, and the correct output might (and most likely will) be very different.

You should write a function Q3_P2() which takes no arguments and generates the file "Q3_Part2.txt" containing all the relevant information. You can choose to write any helper routines in accordance with your design, and your Q3_P2() function may call them in any order.

from bs4 import BeautifulSoup 
import urllib 
import re 
import numpy as np 
import math 
​
def Q3_P2(): 
    # Parsing through each of the stop words
    stopwords = open('stop_words.txt', 'r') 
    swlist = [] 
    for line in stopwords: 
        swlist.append(line.strip().lower()) 
        
    stopwords.close() 
    
    file = open('urls.txt', 'r')  
​
    urlList = []    
​
### Lists to return ###
    uniqueList = [] 
    lengthList = [] 
    tfCovidList = [] 
    tfMaskList = [] 
    tfHealthList = [] 
    idfCovid = 0
    idfMask = 0 
    idfHealth = 0
    tfidfCovidList = [] 
    tfidfMaskList = [] 
    tfidfHealthList = [] 
###---------------##### 
​
    for url in file: 
        urlList.append(url.strip()) 
​
    for url in urlList:  
        webUrl = urllib.request.urlopen(url).read()  
        soup = BeautifulSoup(webUrl, 'lxml') 
        paragraphs = soup.find_all('p')   
        pgraphs = ''
        for p in paragraphs:  
            ptext = p.getText().strip().lower() 
            pgraphs += ptext + ' '  
​
        rec = re.compile(r"\w+|\d+") 
        totalText = pgraphs.strip()    
        words = rec.findall(totalText) 
        wdict = {}  
        lent = 0 
        # filtering through the words, and filtering through the stop words
        for word in words: 
            if word not in swlist: 
                if word not in wdict: 
                    wdict[word] = 1 
                    lent += 1
                else: 
                    wdict[word] += 1 
                    lent += 1
                    
        ddict = sorted(wdict.items(), key=lambda x: x[1])  
        uniquewords = 0   
        covidCount = 0 
        healthCount = 0 
        maskCount = 0
        for tup in ddict:  
            for item in tup:
                if tup[1] == 1: 
                    uniquewords += 1  
                if tup[0] == 'coronavirus': 
                    covidCount += 1 
                if tup[0] == 'mask': 
                    maskCount += 1 
                if tup[0] == 'health': 
                    healthCount += 1 
​
​
    
# # #
# # # the words I'm choosing to analyze for tf, idf, and tf-idf are {'coronavirus', 'mask', 'health'}  
# # #     
        
        uniqueList.append(uniquewords)   
        lengthList.append(lent)   
        
        tfCovid = covidCount / len(words)  
        tfCovidList.append(tfCovid) 
        tfMask = maskCount / len(words) 
        tfMaskList.append(tfMask) 
        tfHealth = healthCount/len(words)
        tfHealthList.append(tfHealth) 
​
        count1 = 0 
        for num in tfCovidList: 
            if (num != 0.0): 
                count1 += 1
        count2 = 0 
        for num in tfMaskList: 
            if (num != 0.0): 
                count2 += 1  
        count3 = 0 
        for num in tfHealthList: 
            if (num != 0.0): 
                count3 +=1
​
​
    a = 'unique: ' + '' + str(uniqueList)  
    b = 'length: ' + '' + str(lengthList) 
    c = 'tf coronavirus: ' + '' + str(tfCovidList)  
    d = 'tf mask: ' + '' + str(tfMaskList) 
    e = 'tf health: ' + '' + str(tfHealthList) 
    idfCovid = np.log((len(urlList) / (1 + count1))) 
​
    idfMask = np.log(len(urlList) / (1 + count2)) 
​
    idfHealth = np.log(len(urlList) / (1 + count3))  
    
    f = 'idf coronavirus: ' + str(idfCovid)
    g = 'idf mask: ' + str(idfMask)  
    h = 'idf health: ' + str(idfHealth)
​
    for num in tfCovidList: 
        tfidfCovidList.append(idfCovid * num) 
​
    for num in tfMaskList: 
        tfidfMaskList.append(idfMask * num) 
​
    for num in tfHealthList: 
        tfidfHealthList.append(idfHealth * num) 
​
    i = 'tf-idf coronavirus: ' + str(tfidfCovidList) 
    j = 'tf-idf mask: ' + str(tfidfMaskList)
    k = 'tf-idf health: ' + str(tfidfHealthList)
​
    q3file = open('Q3_part2.txt', 'w')  
    lineas = [a,b,c,d,e,f,g,h,i,j,k] 
    for line in lineas: 
        q3file.write(line + '\n') 
     
    q3file.close() 
    
    q3file_1 = open('Q3_part2.txt', 'r') 
    for line in q3file_1: 
        print(line.strip()) 
    
    q3file_1.close() 
    return ('')
    
​
print(Q3_P2())  
​
​
unique: [636, 868, 358, 656, 1130, 296, 746, 586]
length: [664, 1102, 382, 830, 1530, 304, 745, 636]
tf coronavirus: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
tf mask: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
tf health: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001352265043948614, 0.0]
idf coronavirus: 2.0794415416798357
idf mask: 2.0794415416798357
idf health: 1.3862943611198906
tf-idf coronavirus: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
tf-idf mask: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
tf-idf health: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0018746374051655044, 0.0]

###
### AUTOGRADER TEST - DO NOT REMOVE
###
​
​

